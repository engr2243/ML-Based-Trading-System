{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9eb9aa75-f4fb-451f-b65b-9de0186fc34a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Relevant Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "032140e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31e110ce-b7e6-4d0d-a362-70564cdc3b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Add Relevant Libraries\n",
    "#################################################\n",
    "from datetime import datetime, time, date, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "CWD = os.getcwd()\n",
    "WD = CWD.split('Main')[0] + 'Main/'\n",
    "DFNC = WD + 'Functions'\n",
    "sys.path.insert(1, DFNC)\n",
    "\n",
    "#### Add External Functions\n",
    "#################################################\n",
    "from Scripts.FN_Support import Drct, Periods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef4bf18-d04d-4bf4-8efb-42dd38380b51",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Updated Patterns\n",
    "- Below is an updated summery of patterns\n",
    "- Several of these patterns are new from what I sent you last time\n",
    "- All of them have a different column naming structure (more on that below...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f622cee-f424-4bcc-b9ae-a03476ef560f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   pcode  filled\n",
      "0   3BPD   17777\n",
      "1   3BPU   18429\n",
      "2   BFLD   13953\n",
      "3   BFLU   14734\n",
      "4    BOD   42491\n",
      "5    BOU   73240\n",
      "6   BTRD    2965\n",
      "7   BTRU    2990\n",
      "8    CCB   26940\n",
      "9    CCS   26629\n",
      "10    DB   69421\n",
      "11   DBB   22744\n",
      "12    DT   28497\n",
      "13   DTS   11382\n",
      "14   PBB     880\n",
      "15   PBS     882\n",
      "16  SBRT   16388\n",
      "17   SSD   87128\n",
      "18  SSRT    6777\n",
      "19   SSU   86691\n",
      "20   TAG   14475\n",
      "21   TAR   14117\n"
     ]
    }
   ],
   "source": [
    "### Set the AGG Period Type\n",
    "#######################################\n",
    "AGG='5T'\n",
    "\n",
    "### Load Pattern Summeries for ALL_CODES\n",
    "#######################################\n",
    "SUMMERY_FILE = f'{WD}Sources/aggs/{AGG}/Results/Pattern_Summeries/All.csv'\n",
    "summeries = Drct.csv_from(SUMMERY_FILE)\n",
    "ALL_CODES = list(summeries[\"pcode\"].unique())\n",
    "print(summeries[['pcode','filled']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814c5414-34a0-4b09-87d6-1d620602d366",
   "metadata": {},
   "source": [
    "### Load Lookback Periods\n",
    "- This LOOKBACK_DF is what I use to re-test the previous 12-month period every 1-month\n",
    "  1. So every month I look at the previous 12-months starting on the last day of the previous month\n",
    "  2. I want to create new models from the previous 12-month period\n",
    "  3. I will backtest/validate the models by running the current month data through the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4f80a93-0119-4789-bc54-099a9b31e913",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   month_id lookback_start lookback_end\n",
      "0   2022_11     2021-11-01   2022-10-31\n",
      "1   2022_10     2021-10-01   2022-09-30\n",
      "2   2022_09     2021-09-01   2022-08-31\n",
      "3   2022_08     2021-08-01   2022-07-31\n",
      "4   2022_07     2021-07-01   2022-06-30\n",
      "5   2022_06     2021-06-01   2022-05-31\n",
      "6   2022_05     2021-05-01   2022-04-30\n",
      "7   2022_04     2021-04-01   2022-03-31\n",
      "8   2022_03     2021-03-01   2022-02-28\n",
      "9   2022_02     2021-02-01   2022-01-31\n",
      "10  2022_01     2021-01-01   2021-12-31\n",
      "11  2021_12     2020-12-01   2021-11-30\n"
     ]
    }
   ],
   "source": [
    "### Load Lookback Periods\n",
    "#######################################\n",
    "LOOKBACK_DF = Periods.lookback_df(LB_MONTHS=12,NEXT=False,MIN_DATE=None,MAX=None,LB_MIN_DATE=None)\n",
    "LOOKBACK_DF = LOOKBACK_DF[['month_id','lookback_start','lookback_end']][(LOOKBACK_DF[\"month_id\"] <= '2022_11')].head(12).reset_index(drop=True)\n",
    "print(LOOKBACK_DF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c9ba8d-5950-47bd-9beb-424b6b2172ca",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model Creation Master Function\n",
    "- This will obviously but a much longer and more involved function but the main takeways are:\n",
    "   - Even though we need to re-create these models, I think it's save to build off of your analysis that Random Forest & All Features seems to work best, so let's limit the model creation to that general scope for now.\n",
    "   - It should save the model information for the specific lookback period into that period's folder\n",
    "   - It should save whatever information is neccesary (i.e. pickle file) so that in production mode it can make as fast of a prediction as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f352b87-71aa-42ce-b928-71178d9b3bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "import sklearn\n",
    "import catboost as cb\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.decomposition import PCA\n",
    "import pickle\n",
    "import pathlib\n",
    "\n",
    "#define your own mse and set greater_is_better=False\n",
    "mse = make_scorer(mean_squared_error,greater_is_better=False)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "def create_model(model_creation_data,AGG,PCODE,MONTH_ID,\n",
    "                 columns_dist, Scaling=None, n_test=0,\n",
    "                 regression='rf'):\n",
    "    print(f\"\\n\\t{'-'*60}\\n\\tCREATE MODEL FOR '{PCODE}' | {MONTH_ID}\\n\\t{'-'*60}\")\n",
    "    ''' ADD FUNCTIONALITY HERE '''\n",
    "    numeric_features = columns_dist['cols_attributes_basic']\n",
    "    cat_features = columns_dist['cols_attributes_category']\n",
    "    column_target = columns_dist['column_target']\n",
    "    \n",
    "    # Standard Scalar Transformer of numeric contineous data\n",
    "    numeric_tr_SS = Pipeline(steps=[(\"scaler\", StandardScaler())])\n",
    "    \n",
    "    # Minmax Transformer of numeric contineous data\n",
    "    numeric_tr_MM = Pipeline(steps=[(\"scaler\", MinMaxScaler())])\n",
    "        \n",
    "    # Catagegorcial transformers on catagorical attributes\n",
    "    categorical_transformer = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "    \n",
    "    # Features and Target data\n",
    "    X = model_creation_data[numeric_features+cat_features]\n",
    "    y = model_creation_data[[column_target]]\n",
    "    \n",
    "    if Scaling:\n",
    "        if Scaling == 'MinMax':\n",
    "            numeric_transformer = numeric_tr_MM\n",
    "        elif scaling=='StandardScalar':\n",
    "            numeric_transformer = numeric_tr_SS\n",
    "        \n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                (\"num_transform\", numeric_transformer, numeric_features),\n",
    "                (\"cat_transform\", categorical_transformer, cat_features),\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "    else:\n",
    "        preprocessor = ColumnTransformer(\n",
    "                transformers=[\n",
    "                    (\"cat_transform\", categorical_transformer, cat_features),\n",
    "                ],\n",
    "                remainder='passthrough')\n",
    "    \n",
    "    pca = PCA()\n",
    "    \n",
    "    param = {\n",
    "        'rf_regressor':{'regressor__n_estimators': [100, 150,\n",
    "                                                       200, 250, 300,\n",
    "                                                       350, 400, 450, 500, 550,\n",
    "#                                                        600,700,800,900,1000\n",
    "                                                      ],\n",
    "                'regressor__max_features':['sqrt', 'log2'],\n",
    "#                 'pca__n_components': [10, 15, 30, 45, 60, 75, 90, 100],\n",
    "                                                       },\n",
    "        'catboost':{'regressor__iterations': [100, 150, 200],\n",
    "                    'regressor__learning_rate': [0.1, 0.2, 0.3, 0.4],\n",
    "                    'regressor__depth': [4, 6, 8, 10],\n",
    "                    'regressor__l2_leaf_reg': [0.1, 0.2, 0.5, 1, 1.5],\n",
    "                    'regressor__verbose': [True]\n",
    "#                 'pca__n_components': [10, 15, 30, 45, 60, 75, 90, 100],                   \n",
    "                   }\n",
    "               \n",
    "               }\n",
    "    if regression == 'rf':\n",
    "        rfRegressor = RandomForestRegressor(random_state=42)\n",
    "        param_grid = param['rf_regressor']\n",
    "        pipe = Pipeline(\n",
    "            steps=[(\"preprocessor\", preprocessor), (\"regressor\", rfRegressor)])\n",
    "    elif regression=='catboost':\n",
    "        catboost = cb.CatBoostRegressor(loss_function='RMSE', task_type=\"GPU\")\n",
    "        param_grid = param['catboost']\n",
    "        pipe = Pipeline(\n",
    "            steps=[(\"preprocessor\", preprocessor), (\"regressor\", catboost)])\n",
    "    elif regression=='DT':\n",
    "        None\n",
    "        #Add code\n",
    "        \n",
    "\n",
    "    search = GridSearchCV(estimator = pipe,\n",
    "                          param_grid=param_grid,\n",
    "                          return_train_score=True,\n",
    "                          scoring=mse,\n",
    "#                           verbose=2,\n",
    "                          refit = True,\n",
    "                          n_jobs=-1)\n",
    "    \n",
    "    print(f\"\\t1. Train Random Forest Model using All Features\")\n",
    "    search.fit(X, y.values.flatten())\n",
    "    best_fit = search.best_estimator_\n",
    "\n",
    "    MODEL_CREATION_DIRECTORY = f'Sources/aggs/{AGG}/Results/Pattern_Models/{PCODE}/test_{n_test}/{MONTH_ID}/'\n",
    "    pathlib.Path(os.path.join(WD, MODEL_CREATION_DIRECTORY)).mkdir(parents=True, exist_ok=True)    \n",
    "    \n",
    "    print(f\"\\t2. Create Model Directory Folder\\n\\t     {MODEL_CREATION_DIRECTORY}\")\n",
    "    print(f\"\\t3. Save Neccesary Files for Fast Predictions in Production\")\n",
    "    pickle.dump(best_fit, open(os.path.join(WD, MODEL_CREATION_DIRECTORY) + 'model.sav', 'wb'))  \n",
    "    print(f\"\\t{'-'*60}\\n\")\n",
    "    return search\n",
    "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "86262d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def analyse_model(model_creation_data, model_production_data,AGG,PCODE,MONTH_ID, n_test=0):\n",
    "    print(f\"\\n\\t{'-'*60}\\n\\tANALYSE MODEL FOR '{PCODE}' | {MONTH_ID}\\n\\t{'-'*60}\")\n",
    "    \n",
    "    numeric_features = columns_dist['cols_attributes_basic']\n",
    "    cat_features = columns_dist['cols_attributes_category']\n",
    "    column_target = columns_dist['column_target']\n",
    "    X_creation_data = model_creation_data[numeric_features+cat_features]\n",
    "    y_creation_data = model_creation_data[[column_target]]\n",
    "    \n",
    "    X_production_data = model_production_data[numeric_features+cat_features]\n",
    "    y_production_data = model_production_data[[column_target]]\n",
    "    \n",
    "    MODEL_CREATION_DIRECTORY = f'Sources/aggs/{AGG}/Results/Pattern_Models/{PCODE}/test_{n_test}/{MONTH_ID}/'\n",
    "    MODEL_PATH = os.path.join(WD, MODEL_CREATION_DIRECTORY, 'model.sav')\n",
    "    \n",
    "    print(f\"\\t1. Load Saved Model Files created from 'model_creation_data'\")\n",
    "    model = pickle.load(open(MODEL_PATH, 'rb'))\n",
    "    \n",
    "    print(f\"\\t2. Running Inference on from 'model_creation_data'\")\n",
    "    prediction_creation_data =  model.predict(X_creation_data)\n",
    "    model_creation_data['predict'] = prediction_creation_data\n",
    "    model_creation_data.insert(1, 'predict', model_creation_data.pop('predict'))\n",
    "    \n",
    "    print(f\"\\t3. Running Inference on from 'model_creation_data'\")\n",
    "    prediction_production_data =  model.predict(X_production_data)\n",
    "    model_production_data['predict'] = prediction_production_data\n",
    "    model_production_data.insert(1, 'predict', model_production_data.pop('predict'))\n",
    "    \n",
    "    mse_cr = mean_squared_error(y_creation_data['_TARGET'].values.flatten(), prediction_creation_data)\n",
    "    mse_pr = mean_squared_error(y_production_data['_TARGET'].values.flatten(), prediction_production_data)\n",
    "    print(f\"\\t4. MSE on model_production_data:{mse_pr}\")\n",
    "    \n",
    "    corr_cr = pearsonr(y_creation_data['_TARGET'].values.flatten(), prediction_creation_data).statistic\n",
    "    corr_pr = pearsonr(y_production_data['_TARGET'].values.flatten(), prediction_production_data).statistic\n",
    "    print(f\"\\t4. MSE on model_production_data:{corr_pr}\")\n",
    "    \n",
    "    result = {'PCODE': PCODE, 'MONTH_ID': MONTH_ID,\n",
    "              'Corr_cr': corr_cr, 'Corr_pr': corr_pr,\n",
    "             'MSE_cr': mse_cr, 'MSE_pr': mse_pr}\n",
    "\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.tight_layout()\n",
    "    sns.set(rc={\"figure.figsize\":(10, 5)})\n",
    "    sns.regplot(x=\"_TARGET\", y=\"predict\", data=model_production_data);\n",
    "    plt.show()\n",
    "    \n",
    "    ## Plot by Date\n",
    "    plt.figure()\n",
    "    sns.set(rc={\"figure.figsize\":(10, 5)})\n",
    "    sns.lineplot(x = \"_UTILITY_date\", y = \"_TARGET\",data = model_creation_data)\n",
    "    sns.lineplot(x = \"_UTILITY_date\", y = \"_TARGET\",data = model_production_data)\n",
    "    sns.lineplot(x = \"_UTILITY_date\", y = \"predict\",data = model_creation_data)\n",
    "    sns.lineplot(x = \"_UTILITY_date\", y = \"predict\",data = model_production_data)\n",
    "    plt.legend(labels=['creation_data_target', 'creation_data_predict',\n",
    "                       'production_data_target', 'production_data_predict',])\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return result\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     ''' ADD FUNCTIONALITY HERE '''\n",
    "    \n",
    "#     print(f\"\\t2. Create Prediction Column in 'model_production_data'\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450caa73-e04d-461a-b934-f76306b72126",
   "metadata": {},
   "source": [
    "### Loop through Lookback Periods\n",
    "- Below is an example of what I would like to be able to do in creating & testing models for each lookback period\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06c09323-c4b0-438c-af0a-dd16729d04ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LOOKBACK_DF' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12181/683594287.py\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mLOOKBACK_DF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# <-- Set to just 1 loop for this example, but I will run this for all periods\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mMONTH_ID\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"month_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlookback_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"lookback_start\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%Y-%m-%d'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LOOKBACK_DF' is not defined"
     ]
    }
   ],
   "source": [
    "ls = []\n",
    "for index, row in LOOKBACK_DF[0:1].iterrows(): # <-- Set to just 1 loop for this example, but I will run this for all periods\n",
    "    \n",
    "    MONTH_ID = row[\"month_id\"]\n",
    "    lookback_start = pd.to_datetime(row[\"lookback_start\"]).strftime('%Y-%m-%d')\n",
    "    lookback_end = pd.to_datetime(row[\"lookback_end\"]).strftime('%Y-%m-%d')\n",
    "    LAST_MO_ID = pd.to_datetime(row[\"lookback_end\"]).strftime('%Y_%m')\n",
    "    print(f\"\\n{'='*100}\\nRun for {MONTH_ID} \\t Lookback Range {lookback_start} to {lookback_end}\\n{'='*100}\\n\")\n",
    "    \n",
    "    for PCODE in ALL_CODES: # <-- Set to just 1 PCODE for this example, but I will run this for all PCODES\n",
    "        \n",
    "        ### Load Production Data (For Backtesting)\n",
    "        #######################################\n",
    "        model_production_data = Drct.load_pcode(AGG=AGG,PCODE=PCODE,PP=True,LAST_MO_ID=MONTH_ID,MONTHS_BACK=1,PRINT=True)\n",
    "        ### Model Creation Data\n",
    "        #######################################\n",
    "        model_creation_data = Drct.load_pcode(AGG=AGG,PCODE=PCODE,PP=True,LAST_MO_ID=LAST_MO_ID,MONTHS_BACK=12,PRINT=True)\n",
    "        model_creation_data['_UTILITY_date'] = pd.to_datetime(model_creation_data['_UTILITY_date'], format='%Y/%m/%d')\n",
    "        model_production_data['_UTILITY_date'] = pd.to_datetime(model_production_data['_UTILITY_date'], format='%Y/%m/%d')\n",
    "\n",
    "        model_production_data['_ftN_time'] = model_production_data['_UTILITY_puid'].apply(lambda x: int(x.split('_')[1]))\n",
    "        model_creation_data['_ftN_time'] = model_creation_data['_UTILITY_puid'].apply(lambda x: int(x.split('_')[1]))  \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "# #         ### Define Column Types\n",
    "# #         #######################################\n",
    "#         cols = model_creation_data.columns.tolist()\n",
    "#         basic_is = lambda x: True in [ i in x for i in ['_ftN_',]]\n",
    "#         category_is = lambda x: True in [ i in x for i in ['_ftC_',]]\n",
    "#         utility_is = lambda x: True in [ i in x for i in ['_UTILITY_',]]\n",
    "#         cols_attributes_basic = [ i for i in cols if basic_is(i) ]\n",
    "#         cols_attributes_category = [ i for i in cols if category_is(i) ]\n",
    "#         cols_utility = [ i for i in cols if utility_is(i) ]\n",
    "#         column_target = '_TARGET'\n",
    "        \n",
    "#         columns_dist = {'cols_attributes_basic': cols_attributes_basic,\n",
    "#                 'cols_attributes_category': cols_attributes_category,\n",
    "#                 'column_target': column_target}\n",
    "        \n",
    "        \n",
    "        ## RUN :: Model Creation\n",
    "        ######################################\n",
    "        \n",
    "#         create_model(model_creation_data,AGG,PCODE,MONTH_ID, columns_dist, n_test=0,regression='rf')\n",
    "        \n",
    "        ### RUN :: Model Production Predict\n",
    "        #######################################\n",
    "#         model_production_data, stats = run_production_predict(model_production_data,AGG,PCODE,MONTH_ID)\n",
    "        \n",
    "        ### RUN :: Model Analysis\n",
    "        #######################################\n",
    "        result = analyse_model(model_creation_data, model_production_data,AGG,PCODE,MONTH_ID, n_test=0)\n",
    "#         ls.append(result)\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e4d7b90a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PCODE': '3BPD',\n",
       " 'MONTH_ID': '2022_11',\n",
       " 'Corr_cr': 0.9923488759079054,\n",
       " 'Corr_pr': 0.11419990133524326,\n",
       " 'MSE_cr': 5.891390299121038e-06,\n",
       " 'MSE_pr': 5.366328144919452e-05}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30ffe57-450d-4550-9507-b8fd14291919",
   "metadata": {},
   "source": [
    "### NOTES About Column Naming Structure\n",
    "- There are 3 primary types of columns:\n",
    "    1. The target column (y) for creating/validating predictions. This column is named '_TARGET'\n",
    "    2. Utility columns that is just here to help me inspect the data but these should not be used in creating predictions (starts with '_UTILITY_')\n",
    "    2. Basic Attribute columns (X) that are numerical (but not neccesarily all linier) used for creating predictions (starts with '_ftN_')\n",
    "    2. Categorical Attribute columns (X) that have already been converted from different strings to intigers used for creating predictions (starts with '_ftC_')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0a0bb37b-f5a8-451d-a863-c8a78a877972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Utility Column Examples \t ['_UTILITY_date', '_UTILITY_puid']\n",
      "Basic Attributes Examples \t ['_ftN_open', '_ftN_high', '_ftN_low', '_ftN_close', '_ftN_volume']\n",
      "Category Attributes Examples \t ['_ftC_gap_type', '_ftC_gap_level', '_ftC_gap_code']\n",
      "Target Column Is \t\t _TARGET\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nUtility Column Examples \\t {cols_utility[0:5]}\")\n",
    "print(f\"Basic Attributes Examples \\t {cols_attributes_basic[0:5]}\")\n",
    "print(f\"Category Attributes Examples \\t {cols_attributes_category[0:5]}\")\n",
    "print(f\"Target Column Is \\t\\t {column_target}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b1fdfe-032f-4276-ae1c-257ee2e87491",
   "metadata": {},
   "source": [
    "### NOTES About model_production_data & model_creation_data\n",
    "- The function Drct.load_pcode() Loads the model data for a specific date range.\n",
    "- Notice the date range for the 'model_production_data' is a 1-month period\n",
    "- Notice the date range for the 'model_creation_data' is the previous 12-month period\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dda6d9b-3914-4363-bee8-364da1b2ff1b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model Production Master Function\n",
    "- This function is meant to be a \"mock version\" of what I will end up using with live real-time data.\n",
    "- In live production mode, I will take real-time data (just like 'model_production_data' except only a few rows at a time and not a whole month) and send it to this function in order to get a new 'predict' column in return.\n",
    "- For backtesting purposes I would also like to have a 'stats' dataframe returned as well with run-time and accuracy scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886909ac-db4f-49d2-9e96-5f65ee35417f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_production_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43abbeaa-21ab-40f7-947f-953b0555f256",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3f8821-f0a2-49ec-9d17-a203fc776775",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e8dce9-0b95-44fd-9b19-21f526507369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "def run_production_predict(model_production_data,AGG,PCODE,MONTH_ID):\n",
    "    \n",
    "    stats_start_time = datetime.now()\n",
    "    \n",
    "    MODEL_CREATION_DIRECTORY = f'Sources/aggs/{AGG}/Results/Pattern_Models/{PCODE}/{MONTH_ID}/'\n",
    "    print(f\"\\n\\t{'-'*60}\\n\\tRUN PRODUCTION PREDICT FOR '{PCODE}' | {MONTH_ID}\\n\\t{'-'*60}\")\n",
    "    \n",
    "    print(f\"\\t1. Load Saved Model Files created from 'model_creation_data'\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(f\"\\t2. Create Prediction Column in 'model_production_data'\")\n",
    "    \n",
    "    ''' ADD FUNCTIONALITY HERE '''\n",
    "    model_production_data['predict'] = np.nan # <-- Add prediction here\n",
    "    model_production_data.insert(1, 'predict', model_production_data.pop('predict'))\n",
    "    \n",
    "    print(f\"\\t3. Create Stats KPI CSV File with 'model_production_data' results\")\n",
    "    \n",
    "    ''' ADD FUNCTIONALITY HERE '''\n",
    "    \n",
    "    stats_run_seconds = (datetime.now() - stats_start_time).total_seconds()\n",
    "    raw = {\n",
    "    'pcode': [PCODE],\n",
    "    'month_id': [MONTH_ID],\n",
    "    'prediction_accuracy': [None], # <-- Something that compares '_TARGET' to 'predict'\n",
    "    'prediction_seconds': [stats_run_seconds], # <-- Something similar to this \n",
    "    }\n",
    "    stats = pd.DataFrame(data=raw)\n",
    "    \n",
    "    print(f\"\\t4. Save the updated 'model_production_data' and 'stats' to CSV\")\n",
    "    \n",
    "    print(f\"\\t5. Return the updated 'model_production_data' and 'stats' from function\")\n",
    "    \n",
    "    print(f\"\\t{'-'*60}\\n\")\n",
    "    \n",
    "    return model_production_data, stats\n",
    "          \n",
    "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fad7244b-49bd-4871-80e5-b4a2952974c5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'run_production_predict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12607/506616812.py\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m### RUN :: Model Production Predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#######################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel_production_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_production_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_production_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mAGG\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mPCODE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mMONTH_ID\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'run_production_predict' is not defined"
     ]
    }
   ],
   "source": [
    "### RUN :: Model Production Predict\n",
    "#######################################\n",
    "model_production_data, stats = run_production_predict(model_production_data,AGG,PCODE,MONTH_ID)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188ebed1-8c6f-4967-8d3c-83f4be39f278",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6529c4-8a62-4cd9-ac2a-0a5d40548146",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_production_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0437d247",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost.utils import get_gpu_device_count\n",
    "print('I see %i GPU devices' % get_gpu_device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19220b9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
