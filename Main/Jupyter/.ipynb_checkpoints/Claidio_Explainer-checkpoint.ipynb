{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8ec39ec-ce88-4021-9ed9-122abe40577d",
   "metadata": {},
   "source": [
    "# Claudio: Explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f7ab33-c87c-40ce-a900-0af9afcccb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, time, date, timedelta\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "CWD = os.getcwd()\n",
    "SL = '/' if CWD[0] == '/' else '\\ '[0]\n",
    "WD = CWD.split('Main')[0] + 'Main' + SL\n",
    "DFNC = WD + 'Functions'\n",
    "sys.path.insert(1, DFNC)\n",
    "\n",
    "from Claudio_Main import *\n",
    "from Claudio_Chart_Examples import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef4bf18-d04d-4bf4-8efb-42dd38380b51",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Pattern Data Overview\n",
    "- The data I am analyzing is comprised of 15 different technical patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88c8dc1-2014-4b02-92db-ff0483ea6721",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_codes = patterns_source(FETCH=False,DF='codes')\n",
    "print(pattern_codes[['pcode','pname',]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fb94ae-96d1-4d19-8d89-1888abdcaae9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chart_data,chart_results = chart_recent(PUID='DB_202003121145_5T_ADM',CHART=500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b87a0b-a5aa-4ca7-9fa0-29b269c24fc9",
   "metadata": {},
   "source": [
    "- Each found pattern has a row of data like the below example from a \"Double Bottom\" (DB)\n",
    "- There are between 300 and 400 column attributes for each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ee90cc-6396-489b-a53f-19b04190b8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009a517a-2c56-40f2-90d1-80ab023c53dd",
   "metadata": {},
   "source": [
    "- And those rows are separated by PCODE saved to separate CSV Summery Files in the \"Patterns_All\" folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbae3b0-9d28-4dc3-8354-359afe5aa2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "PCODE = 'DB'\n",
    "CSV_PCODE = f'{WD}Sources{SL}aggs{SL}5T{SL}Results{SL}Patterns_All{SL}{PCODE}.csv'\n",
    "data_pcode = csv_from(CSV_PCODE,LM=True)\n",
    "print(f\"\\nThe 'Double Bottom' summery file, for example, has {len(data_pcode):,.0f} rows (and {data_pcode.shape[1]} attributes) between {data_pcode['date'].min()} and {data_pcode['date'].max()}\\n\")\n",
    "\n",
    "data_pcode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad501eb-385a-4f18-aafa-486bdfb5e1b9",
   "metadata": {},
   "source": [
    "- Out of the 300 to 400 columns for each pattern (PCODE):\n",
    "  - some are support columns (i.e. pattern found time, data fetch time, etc), \n",
    "  - some are future biased results (i.e. return, exit datetime, etc), \n",
    "  - and most are technical backwards looking attributes that I am trying to create predictions from (i.e. open, high, low, volume, rsi, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ec4d31-7e63-4c0d-802c-5e4c49d456f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_cols = patterns_source(FETCH=False,DF='cols')\n",
    "print(f\"SUPPORT COLUMNS\\n{pattern_cols[['column',]][(pattern_cols['category'] == 'bars') & (pattern_cols['w_col'] == False)].head(4)}\\n\")\n",
    "print(f\"FUTURE BIAS COLUMNS\\n{pattern_cols[['column',]][(pattern_cols['category'] == 'ptrn') & (pattern_cols['bias'] == True)].head(6)}\\n\")\n",
    "print(f\"TECHNICAL UN-BIASED COLUMNS\\n{pattern_cols[['column','rtype','rel_from']][(pattern_cols['w_col'] == True) & (pattern_cols['bias'] == False)].head(6)}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd69aef-d123-4d48-8082-c58a6c564bc8",
   "metadata": {},
   "source": [
    "- The raw PCODE data also includes failed patterns (i.e. data['filled'] == 0), and patterns that filled, but would be infeasable to actually execute a trade with (data['otf_minutes'] == 0) where 'otf_minutes' means \"order to fill minutes\").\n",
    "- I decided to leave these rows in the dataset in case they are helpful in feature selection.\n",
    "- I use the function primary_filter() to filter the data down to something closer to what I could practically execute a trade with.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce7cb95-ca35-4202-b8dc-4be4ebe3b64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = primary_filter(data_pcode,PRINT=True,FLD_ANY=True,FLD_SAME_DAY=False,OTF_MINUTES_MIN=0)\n",
    "filtered.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3aa70d-6074-47b9-90e2-6f2780243e1c",
   "metadata": {},
   "source": [
    "- Example of <b>Unfilled</b> \"Double Bottom\" (DB) Pattern (data['filled'] == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07991f5-e472-4594-ba37-ebf517e8d3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_data,chart_results = chart_recent(PUID='DB_202003161215_5T_A',CHART=500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0469629-042a-42f4-a4b6-0e51287bff37",
   "metadata": {
    "tags": []
   },
   "source": [
    "- Example of <b>Filled</b> \"Double Bottom\" (DB) Pattern with a <b>Negative Return</b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fa23c0-dacc-43f5-9864-6952818e7f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_data,chart_results = chart_recent(PUID='DB_202003161115_5T_A',CHART=500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e8fa61-e4fa-44ea-90db-6a1dd5cb9b3b",
   "metadata": {
    "tags": []
   },
   "source": [
    "- Example of <b>Filled</b> \"Double Bottom\" (DB) Pattern with a <b>Positive Return</b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9bfd04-8c37-4a23-a3c0-5cf88dcdf6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_data,chart_results = chart_recent(PUID='DB_202003121145_5T_ADM',CHART=500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea26d9fc-aa2e-4c40-b767-457df1850354",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "56a9b293-ed9f-4705-aa93-cea4c0f3718c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Backtesting Timeframe Methodology\n",
    "- My overall backtesting process is to look at 1-month timeframe against the previous 12-months.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80df9cdc-d5f2-4889-af59-4b66ce053ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOOKBACK_DATES = lookback_df(LB_MONTHS=12,NEXT=False)\n",
    "print(LOOKBACK_DATES[['month_id','lookback_start','lookback_end','backtest_start','backtest_end']][1:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afaa2bc-61a1-4db6-8add-19b00c2bcaeb",
   "metadata": {},
   "source": [
    "- The \"Period_Data\" Folder is comprised of all \"PCODE\" data appended together and separated by MONTH_ID's to make it easier to run my backtesting by MONTH_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40e23ed-6cdb-4863-a5f1-9d72702e2baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "MONTH_ID = '2022_10'\n",
    "LB_ID = LOOKBACK_DATES[(LOOKBACK_DATES[\"month_id\"] == MONTH_ID)].iloc[0]\n",
    "CSV_PERIOD = f'{WD}Sources{SL}aggs{SL}5T{SL}Results{SL}Period_Data{SL}{MONTH_ID}.csv'\n",
    "data_period = csv_from(CSV_PERIOD,LM=True)\n",
    "\n",
    "print(f\"\\nThe below 'data_period' has a date range between {LB_ID.backtest_start.strftime('%Y-%m-%d')} and {LB_ID.backtest_end.strftime('%Y-%m-%d')}\")\n",
    "print(f\"so it would use data between {LB_ID.lookback_start.strftime('%Y-%m-%d')} and {LB_ID.lookback_end.strftime('%Y-%m-%d')} to create predictions from sklearn\\n\")\n",
    "\n",
    "data_period[['pcode','pname','date','filled','return']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3e59d4-f245-43fe-969d-a1034d8332f4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Current Feature Selection Process\n",
    "#### feature_selection_loop()\n",
    "- The current way I go about selecting the ideal features for making pattern predictions (which I believe has a lot of room for improvement) is through the function <b>feature_selection_loop()</b>\n",
    "- The gists of the function is to loop through the different lookback timeframes, and then for each timeframe it loops through the patterns by 'pcode' and attempts to find the ideal attribute columns for each pattern\n",
    "- It then saves that data to a separate CSV file for future backtesting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b60f6ac-88ec-444c-a524-13d2f2df2c5e",
   "metadata": {},
   "source": [
    "#### Step 1 :: Create Looping Route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22422aad-1a23-4bd5-8880-088fb64ec490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "#def feature_selection_loop(OFFSET=None,AGG='5T',MAX_MO_IDS=26,MO_IDS=[],MO_ID_IS=None,PCODE_LST=[]):\n",
    "OFFSET=None\n",
    "AGG='5T'\n",
    "MAX_MO_IDS=100\n",
    "MO_IDS=[]\n",
    "MO_ID_IS='2022_10' # <<<< I'm limiting it to just this MO_ID for the sake of this example\n",
    "PCODE_LST=['DB'] # <<<< I'm limiting it to just this PCODE for the sake of this example\n",
    "\n",
    "#### Get Possible MO_IDS\n",
    "#################################################\n",
    "if MO_ID_IS != None:\n",
    "    MO_IDS = lookback_df(LB_MONTHS=12,NEXT=True)\n",
    "    MO_IDS = MO_IDS[(MO_IDS[\"month_id\"] == MO_ID_IS)]\n",
    "elif len(MO_IDS) == 0:\n",
    "    PERIOD_DATA_FOLDER = f'{WD}Sources{SL}aggs{SL}{AGG}{SL}Results{SL}Period_Data'\n",
    "    files = glob.glob(f\"{PERIOD_DATA_FOLDER}{SL}*.csv\")\n",
    "    if len(files) > 0:\n",
    "        MO_IDS = pd.DataFrame(files)\n",
    "        MO_IDS['month_id'] = MO_IDS[0].astype(str)\n",
    "        MO_IDS['month_id'] = MO_IDS['month_id'].str.split('Period_Data', expand=True)[1].str.split('.csv', expand=True)[0].str[1:]\n",
    "        MO_IDS = MO_IDS[['month_id']]\n",
    "MO_IDS = MO_IDS.sort_values(by=['month_id',], ascending=[False]).reset_index(drop=True)\n",
    "MO_IDS = MO_IDS.head(MAX_MO_IDS)\n",
    "\n",
    "#### Get Lookback Dates\n",
    "#################################################\n",
    "LB_12 = lookback_df(LB_MONTHS=12,NEXT=True)\n",
    "LB_24 = lookback_df(LB_MONTHS=24,NEXT=True)\n",
    "LB_12 = LB_12[['month_id','lookback_id','lookback_start','lookback_end']]\n",
    "LB_24 = LB_24[['month_id','lookback_id','lookback_start','lookback_end']]\n",
    "LB_12.rename(columns={'lookback_start': 'LB_12_START','lookback_end': 'LB_12_END','lookback_id': 'LB_12_ID',}, inplace=True)\n",
    "LB_24.rename(columns={'lookback_start': 'LB_24_START','lookback_end': 'LB_24_END','lookback_id': 'LB_24_ID',}, inplace=True)\n",
    "LOOKBACK_DATES = LB_12.merge(LB_24, how = 'inner', on = ['month_id'])\n",
    "LOOKBACK_DATES = LOOKBACK_DATES.merge(MO_IDS, how = 'inner', on = ['month_id'])\n",
    "LOOKBACK_DATES = set_offset(LOOKBACK_DATES,OFFSET)\n",
    "\n",
    "#### Get All Pattern Codes (PCODES)\n",
    "#################################################\n",
    "codes = patterns_source(DF='codes')\n",
    "ALL_CODES = list(codes[\"pcode\"].unique())\n",
    "ALL_CODES = PCODE_LST if len(PCODE_LST) >0 else ALL_CODES\n",
    "\n",
    "#### Itterate through Lookback Dates\n",
    "#################################################\n",
    "count_total = len(LOOKBACK_DATES)\n",
    "count = 0\n",
    "START_time = datetime.now()\n",
    "print(f\"\\n\\n{'='*100}\\n{datetime.now():%I:%M:%S} | Feature Selection Started for {len(LOOKBACK_DATES)} Periods\\n{'='*100}\\n\")\n",
    "\n",
    "for index, row in LOOKBACK_DATES.iterrows():\n",
    "    MONTH_ID = row[\"month_id\"]\n",
    "    LB_12_START = pd.to_datetime(row[\"LB_12_START\"]).strftime('%Y-%m-%d')\n",
    "    LB_12_END = pd.to_datetime(row[\"LB_12_END\"]).strftime('%Y-%m-%d')\n",
    "    LB_24_START = pd.to_datetime(row[\"LB_24_START\"]).strftime('%Y-%m-%d')\n",
    "    LB_24_END = pd.to_datetime(row[\"LB_24_END\"]).strftime('%Y-%m-%d')\n",
    "    \n",
    "    #### Itterate through Pattern Codes (PCODES)\n",
    "    #################################################\n",
    "    for PCODE in ALL_CODES:  # <<<< Limiting it to one PCODE for this example\n",
    "        \n",
    "        ### PCODE Variables/Settings\n",
    "        #######################################\n",
    "        print(f\"{datetime.now():%I:%M:%S} | {MONTH_ID} | {PCODE} \\t Started...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73494630-fd0e-4853-9777-99bf69b4f58c",
   "metadata": {},
   "source": [
    "#### Step 2 :: Load & Filter PCODE Data by 24-Month Data Range\n",
    "- After loading the PCODE data I then cut_by_dates() to filter it to the previous 24-month period.\n",
    "- I typically try to use a 12-month lookback period, but I've found that for this step in the process, 24 month of data seems to perform better\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4216d071-4675-4f0d-86a8-935437b57097",
   "metadata": {},
   "outputs": [],
   "source": [
    "### PCODE Variables/Settings\n",
    "#######################################\n",
    "print(f\"{datetime.now():%I:%M:%S} | {MONTH_ID} | {PCODE} \\t Started...\")\n",
    "CSV_IMPORT_PCODE = f'{WD}Sources{SL}aggs{SL}{AGG}{SL}Results{SL}Patterns_All{SL}{PCODE}.csv'\n",
    "CSV_EXPORT_PCODE = f'{WD}Sources{SL}aggs{SL}{AGG}{SL}Results{SL}Attributes{SL}Data{SL}{MONTH_ID}{SL}{PCODE}.csv'\n",
    "\n",
    "### Load PCODE Data & Filter to Previous 24-Month Period\n",
    "#######################################\n",
    "data = csv_from(CSV_IMPORT_PCODE,LM=False)\n",
    "dates_shape_pre = data.shape\n",
    "data = cut_by_dates(data,CUT_COL='date',CUT_START=LB_24_START,CUT_END=LB_24_END,RESET_INDEX=True)\n",
    "dates_shape_post = data.shape\n",
    "print(f\"{datetime.now():%I:%M:%S} | {MONTH_ID} | {PCODE} \\t Cut-by-Dates \\t\\t {dates_shape_pre} > {dates_shape_post} | {LB_24_START} > {LB_24_END}\")\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a162cffc-8090-4f37-878e-37b03e5ac514",
   "metadata": {},
   "source": [
    "#### Step 3 :: Filter PCODE Data by Patterns that Filled\n",
    "- Then I apply the primary_filter() to mostly filter it down the patterns that filled\n",
    "- I had previously assumed that filtering it even further to something that more identically resembled what I could practically trade on would be better, but in my tests it performed worse. I am hoping to either understand why that is or to have your solution filter it more accurately.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb7894a-2f8c-49bd-8218-1d898f1f16a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_shape_pre = data.shape\n",
    "data = primary_filter(data,PRINT=False,FLD_ANY=True,FLD_SAME_DAY=False,OTF_MINUTES_MIN=0,OTF_MINUTES_MAX=None,RTR_MIN=0,FORCE_EOD=False,GAP_MIN=None,GAP_MAX=None,TIME_HM_MAX=None)\n",
    "filter_shape_post = data.shape\n",
    "print(f\"{datetime.now():%I:%M:%S} | {MONTH_ID} | {PCODE} \\t Primary Filter \\t {filter_shape_pre} > {filter_shape_post}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b274f36-0cf0-48ec-9931-3498087c5cd4",
   "metadata": {},
   "source": [
    "#### Step 4 :: Filter PCODE Data by pattern_data_sklearn_prep()\n",
    "- Before running the <b>pattern_data_sklearn_prep()</b> I set the 'TARGET_COL' (or y value) to 'return', which will be the only future-bias column going forward.\n",
    "- Then inside of the <b>pattern_data_sklearn_prep()</b> function the first step is to load the <b>unbiased_pattern_attributes(PCODE=PCODE)</b>\n",
    "- It then creates a new dataframe based on the atb_df columns 'rtype' and 'rel_from'\n",
    "   - If the 'rtype' == 'self' then it just copies the values in hte PCODE data for 'column'\n",
    "      - i.e. 'rsi_14' = 'rsi_14\n",
    "   - If the 'rtype' != 'self' then it divides the PCODE column data by the PCODE 'rel_from' data, creating a 'relative attribute column'\n",
    "      - i.e. 'volume' = ('volume'/'r20vma')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1f15f1-3918-40d2-8357-1fb2fed952ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "atb_df = unbiased_pattern_attributes(PCODE=PCODE)\n",
    "print(f\"\\nUNBIASED COLUMNS: Non-Relative: {len(atb_df[(atb_df['rtype'] == 'self')])} total\\n{atb_df[(atb_df['rtype'] == 'self')][4:6]}\")\n",
    "print(f\"\\nUNBIASED COLUMNS: Yes-Relative: {len(atb_df[(atb_df['rtype'] != 'self')])} total\\n{atb_df[(atb_df['rtype'] != 'self')][3:5]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda0523d-843b-4787-b412-c5a788824ffb",
   "metadata": {},
   "source": [
    " - The next step is to filter the data down to the top 'COR_MAX_COLS' columns (I've been using 100 as the value) with the greated correlation (which is just to help speed up the next step in the process which is backwards_elimination()\n",
    " - For the sake of this Jupyter example I'm limiting it to the top 10 just to make it faster\n",
    " - I also keep the 'date' column in here so that I can cut_by_date later, but it will be ignored in sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ec7396-2d1a-4476-b7ad-b2ecbc87e0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prep PCODE Data for SKLEARN\n",
    "#######################################\n",
    "TARGET_COL = 'return' # <<<< This is the ONLY future-bias column and will serve as the 'y' value\n",
    "MAX_COLS = 10 # <<<< Referrrs to the max number of columns sorted by correlation (typically set to 100)\n",
    "sklearn_prep_shape_pre = data.shape\n",
    "data = pattern_data_sklearn_prep(data,TARGET_COL=TARGET_COL,PCODE=PCODE,COR_MAX_COLS=MAX_COLS,PRINT=False)\n",
    "sklearn_prep_shape_post = data.shape\n",
    "print(f\"{datetime.now():%I:%M:%S} | {MONTH_ID} | {PCODE} \\t Sklearn Data Prep \\t {sklearn_prep_shape_pre} > {sklearn_prep_shape_post}\")\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85dc2247-7f9e-4899-be96-ee2f275ddcd1",
   "metadata": {},
   "source": [
    "#### Step 5 :: Filter PCODE Data by backward_elimination()\n",
    "- Next I run the backward_elimination() Function to try and select the best attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8674c209-b7ec-41f1-ab4f-a710ab2154d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get Columns via Backward Elimination\n",
    "#######################################\n",
    "be_shape_pre = data.shape\n",
    "BE_COLS,drop_df = backward_elimination(data,TARGET_COL=TARGET_COL,EXCLUDE_COLS=['date'],PRINT=True)\n",
    "EXPORT_DROP_DF = f'{WD}Sources{SL}aggs{SL}{AGG}{SL}Results{SL}Attributes{SL}Drops{SL}{MONTH_ID}{SL}{PCODE}.csv'\n",
    "csv_to(drop_df,EXPORT_DROP_DF)\n",
    "PDATA_COLS = ['date',TARGET_COL] + BE_COLS\n",
    "data = data[PDATA_COLS]\n",
    "be_shape_post = data.shape\n",
    "print(f\"{datetime.now():%I:%M:%S} | {MONTH_ID} | {PCODE} \\t Backwards Elimination \\t {be_shape_pre} > {be_shape_post}\")\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbddbba5-b1f9-435f-a0d2-f547e6fe545d",
   "metadata": {},
   "source": [
    "#### Step 6 :: Cut-by-Dates Again to get down to 12-Month Period\n",
    "- Lastly I cut it down to the last 12-month period since this is what I use for all of my backtesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1c7226-6685-46b5-b45a-0599855ac65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Filter Data to to Previous 12-Month Period\n",
    "#######################################\n",
    "dates_shape_pre = data.shape\n",
    "data = cut_by_dates(data,CUT_COL='date',CUT_START=LB_12_START,CUT_END=LB_12_END,RESET_INDEX=True)\n",
    "dates_shape_post = data.shape\n",
    "print(f\"{datetime.now():%I:%M:%S} | {MONTH_ID} | {PCODE} \\t Cut-by-Dates \\t\\t {dates_shape_pre} > {dates_shape_post} | {LB_12_START} > {LB_12_END}\")\n",
    "data = data.sort_values(by=['date',], ascending=[True]).reset_index(drop=True)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1df3c79-bea5-4eb1-ba71-d8531f46661a",
   "metadata": {},
   "source": [
    "#### Step 7 :: Export Attribute Data\n",
    "- Finally, for each PCODE in ALL_CODES, it exports the selected feature data.\n",
    "- Note that if you are testing this with changes, it won't export the file if it already exists so just delete it before running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adf5e54-c721-40cd-b97e-d8ed4f3bfe0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Export Final Data\n",
    "#######################################\n",
    "if os.path.exists(CSV_EXPORT_PCODE):\n",
    "    print(f\"{datetime.now():%I:%M:%S} | {MONTH_ID} | {PCODE} \\t NOT Exported because File Already Exists!\\n\")\n",
    "else:\n",
    "    csv_to(data,CSV_EXPORT_PCODE)\n",
    "    print(f\"{datetime.now():%I:%M:%S} | {MONTH_ID} | {PCODE} \\t Features Export Final \\t {data.shape}\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c9ba8d-5950-47bd-9beb-424b6b2172ca",
   "metadata": {
    "tags": []
   },
   "source": [
    "## get_weights() Function\n",
    "- At the end of each timeframe inside of the <b>feature_selection_loop()</b> it runs the <b>get_weights()</b> function, which essentially applies the historical feature selection data to to current data (see example below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa24422e-b8e5-41a9-abe3-38ad240a2342",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOOKBACK_DATES = lookback_df(LB_MONTHS=12,NEXT=False)\n",
    "print(LOOKBACK_DATES[['month_id','lookback_start','lookback_end','backtest_start','backtest_end']].iloc[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566d029c-137f-4621-8f60-e53d0ffb57e9",
   "metadata": {},
   "source": [
    "- For example, the month_id \"2022_10\" data with a date range between \"backtest_start\" (2022-10-01) and  \"backtest_end\" (2022-10-31) will create a prediction from the  <b>feature_selection_loop()</b> data from date ranges between \"lookback_start\" (2021-10-01) and  \"lookback_end\" (2022-09-30)\n",
    "- The reason I separated them out like this and used local file lookups is to make it run faster on my LIVE script, which takes pattern data from the last five minutes and tries to analyze it to determine whether or not to execute a trade."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7767697c-5123-4218-8b5f-66d39d22f126",
   "metadata": {},
   "source": [
    "- The first step in the <b>get_weights()</b> function is to load the \"present-tense\" data from the \"Period_Data\" folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1026d9-e09c-4a4f-9c9f-86686954d5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "HIST_MONTH = '2022_10'\n",
    "CSV_BT_DATA = f'{WD}Sources{SL}aggs{SL}{AGG}{SL}Results{SL}Period_Data{SL}{HIST_MONTH}.csv'\n",
    "results = csv_from(CSV_BT_DATA,LM=False)\n",
    "results.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9dd7d7-8cbd-4be1-a5b9-4c799b11b19c",
   "metadata": {},
   "source": [
    "- Then in the <b>get_weights()</b> >> <b>prep_weight_data()</b> it will remove the \"return\" column (since it should be blind to that) and create a new X values from the columns exported in the <b>feature_selection_loop()</b> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b982e5-6db7-4893-b6ac-2a33409df13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Get Lookback Data\n",
    "#################################################\n",
    "LB_DATA_CSV = f'{WD}Sources{SL}aggs{SL}{AGG}{SL}Results{SL}Attributes{SL}Data{SL}{MONTH_ID}{SL}{PCODE}.csv'\n",
    "LB_DATA = csv_from(LB_DATA_CSV)\n",
    "LB_DATA.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ceae9f-42eb-474d-8ea5-59c17811afb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "LB_DATA_COLS = LB_DATA.copy()\n",
    "LB_DATA_COLS = LB_DATA_COLS.drop(['date','return',], axis=1, errors='ignore')\n",
    "LB_DATA_COLS = LB_DATA_COLS.head(1).T.reset_index()\n",
    "LB_DATA_COLS.rename(columns={'index': 'column',}, inplace=True)\n",
    "LB_DATA_COLS = LB_DATA_COLS[['column']]\n",
    "LB_DATA_COLS.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8850a1c-ca48-4237-896f-81eb64f08677",
   "metadata": {},
   "source": [
    "- Then in the <b>get_weights()</b> >> <b>calculate_weights()</b> it will use <b>sklearn LinearRegression()</b> to create a prediction, which I call the <b>'weight'</b>\n",
    "- It will also create <b>'w_score'</b> column, which is essentially an easier to read score of the weight (between 0 and 100).\n",
    "- Then it will export the results with the 'weight' and 'w_score' to CSV, which will become the source data for my backtesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3baca363-586c-4f3a-9f81-aff848e20b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "LB_ID = MONTH_ID + '_m12'\n",
    "WEIGHT_EXPORT_CSV = f'{WD}Sources{SL}aggs{SL}{AGG}{SL}Results{SL}Pattern_Weights{SL}{LB_ID}.csv'\n",
    "weights = csv_from(WEIGHT_EXPORT_CSV)\n",
    "\n",
    "weights[['datetime','pcode','puid','weight','w_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ba3517-786b-4ce2-bffd-1a5cb5718fd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "230143a1-3be4-47ec-b739-d7211f604c8e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## backtest_loop_simple() Function\n",
    "- Then in the <b>backtest_loop_simple()</b> function is what I use to apply my current <b>Trading Strategy</b> to the <b>Pattern_Weights data</b> created in the <b>get_weights()</b> function\n",
    "- My <b>Trading Strategy</b> is essentially to:\n",
    "  1. Create a \"POSSIBLES\" dataframe of trades that have a minimum weight value, along with other filtering criteria from the primary_filter() function.\n",
    "  2. Then it sorts those \"POSSIBLES\" by 'datetime', and then by 'w_score' and goes \"all-in\" on the next best opportunity.\n",
    "  3. If that trade does not trigger either a 'target' or 'loss' by the end of the day, then it exits at the end of the day at the close price.\n",
    "  4. As soon as it exits a trade it immedialty looks for the next \"POSSIBLE\" and enters as soon as one hits the filtered criteria.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc4e194-7ff7-4db1-8d35-9c664854bbf0",
   "metadata": {
    "tags": []
   },
   "source": [
    "<b>Below Print Stats Key</b>\n",
    "- PL/Day = Average Profit Percent Per Day\n",
    "- PL/Period = Average Profit Percent Per Period (month)\n",
    "- TPD = Trades Per Day\n",
    "- PPD = Possibles Per Day\n",
    "- P_Mean = Average Profit Percent Per Possible\n",
    "- Wins = Percent of Trades with positive profit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1db9959-16bf-4098-8745-b0201568a1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "POSSIBLES,TRADES,STATS,DAYS = backtest_loop_simple(AGG_LIST=['5T'],BT_LOOPS=6,MO_IDS=[],MO_ID_IS=None,CAP_EXTREMES=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7763652c-b520-49dc-9f34-95a2a88c3e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Over the above period of {len(DAYS):,.0f} days there were {len(POSSIBLES):,.0f} Possible Trades\\n\")\n",
    "POSSIBLES[['datetime','otf_minutes','filled_dt','exit_dt','symbol','pcode','weight','w_score','pos','entry','loss','target','rtr','return','pl_pct',]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0278059-62e1-464a-983f-a6b54eb1870f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"And the Strategy executed {len(TRADES):,.0f} Trades for an average of {STATS['PLPCT_DAY'].iloc[-1]:.2f}% profit/day\\n\")\n",
    "TRADES[['datetime','otf_minutes','filled_dt','exit_dt','symbol','pcode','weight','w_score','pos','entry','loss','target','rtr','return','pl_pct',]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91fe88c-4765-4314-9fb7-16773984bea2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
